[ä¸­æ–‡ç‰ˆæœ¬](README.md)ï¼ŒğŸ‘‰Version FranÃ§aise

# **RÃ©sumÃ© dâ€™avancement du projet Whisper-Hailo8L**

### **Objectif du projet**

Mon objectif Ã  long terme est de permettre Ã  Whisper de fonctionner en temps rÃ©el sur une plateforme composÃ©e dâ€™un Raspberry Pi 5 et dâ€™un accÃ©lÃ©rateur Hailo-8L. Pour atteindre ce rÃ©sultat, il ne sâ€™agit pas simplement de quantifier le modÃ¨le original : je dois reconstruire un encodeur entiÃ¨rement compatible avec les contraintes matÃ©rielles du NPU, puis transfÃ©rer les connaissances du modÃ¨le maÃ®tre vers ce nouvel encodeur au moyen dâ€™une distillation.
 Autrement dit, je ne fais pas simplement du dÃ©ploiement de Whisper, mais je conÃ§ois une version Â« Whisper-Lite Â» adaptÃ©e Ã  Hailo.

### **Conception de lâ€™encodeur et du dÃ©codeur**

**Encodeur**

Dans la premiÃ¨re version, jâ€™ai conÃ§u un encodeur reposant sur une attention linÃ©aire : lâ€™idÃ©e Ã©tait de remplacer lâ€™attention classique par une kernel attention afin dâ€™Ã©viter softmax et les multiplications matricielles volumineuses, difficiles Ã  gÃ©rer sur Hailo. Les deux premiÃ¨res couches convolutionnelles ont Ã©galement Ã©tÃ© modifiÃ©es, et la sortie temporelle a Ã©tÃ© fixÃ©e Ã  500 pas au lieu des 1500 de Whisper-small.

MÃªme si cette structure fonctionnait correctement en PyTorch, elle a introduit un problÃ¨me majeur lors de la distillation. En modifiant la longueur du contexte, les positions temporelles du modÃ¨le enseignant ne correspondaient plus Ã  celles de mon modÃ¨le. La distillation forÃ§a donc lâ€™Ã©tudiant Ã  imiter des reprÃ©sentations latentes dÃ©jÃ  tronquÃ©es. Le processus finissait par converger, mais lâ€™encodeur appris ne reflÃ©tait plus fidÃ¨lement les reprÃ©sentations de Whisper.

Lorsque jâ€™ai tentÃ© de porter cet encodeur sur Hailo, les difficultÃ©s se sont amplifiÃ©es. Le compilateur de Hailo modifie activement le graphe computationnel : LayerNorm devient GroupNorm accompagnÃ© de reshapes, les Ã©tapes de normalisation dans la kernel attention sont rÃ©organisÃ©es, certains paddings sont supprimÃ©s et des reshapes fusionnÃ©s ou Ã©liminÃ©s. AprÃ¨s quantification, le graphe rÃ©sultant nâ€™Ã©tait plus du tout celui que jâ€™avais entraÃ®nÃ© en PyTorch, et les rÃ©sultats dâ€™infÃ©rence divergeaient fortement.
 Lors de certaines tentatives, la phase de quantification consommait mÃªme plusieurs centaines de gigaoctets de mÃ©moire, ce qui montre clairement que cette architecture nâ€™est pas compatible avec lâ€™optimisation interne du compilateur Hailo.

**DÃ©codeur**

La partie dÃ©codeur a posÃ© des problÃ¨mes encore plus importants. Jâ€™ai dâ€™abord essayÃ© un dÃ©codeur basÃ© sur le CTC, mais celui-ci est incapable dâ€™apprendre le contexte linguistique et a tendance Ã  produire beaucoup de tokens Â« blancs Â». Cela contredit complÃ¨tement le rÃ´le du dÃ©codeur dans Whisper, qui agit comme un vÃ©ritable modÃ¨le de langage.

Jâ€™ai ensuite expÃ©rimentÃ© des approches inspirÃ©es de Mamba et des SSM. Lâ€™espoir Ã©tait dâ€™utiliser leurs propriÃ©tÃ©s convolutionnelles et rÃ©currentes pour Ã©viter les calculs dâ€™attention auto-rÃ©gresseurs. Cependant, la thÃ©orie de Mamba repose sur un noyau de convolution de longueur potentiellement infinie. Sur Hailo, je suis obligÃ© de fixer une longueur artificielle pour ce noyau : le modÃ¨le perd alors sa nature rÃ©cursive et se transforme en un simple bloc convolutionnel de taille figÃ©e. MathÃ©matiquement, cela dÃ©nature complÃ¨tement le mÃ©canisme, et pendant la quantification, le comportement du modÃ¨le sâ€™Ã©loigne fortement de celui observÃ© durant lâ€™entraÃ®nement.

Les choses deviennent encore plus complexes au niveau de la cross-attention. MÃªme en remplaÃ§ant softmax par un noyau linÃ©aire, les produits QK nÃ©cessitent une correspondance stricte des formes. DÃ¨s quâ€™un padding ou un broadcast est impliquÃ©, le compilateur Hailo rÃ©Ã©crit automatiquement la structure du graphe. Ã€ partir de ce moment-lÃ , le modÃ¨le obtenu ne peut plus rester cohÃ©rent avec lâ€™original.
 Cela signifie quâ€™un dÃ©codeur basÃ© sur lâ€™attention est, de fait, impossible Ã  implÃ©menter correctement sur Hailo.

**DÃ©cision : laisser le dÃ©codeur sur CPU**

Face Ã  ces constats, jâ€™ai compris que le dÃ©codeur nâ€™est pas adaptÃ© Ã  une exÃ©cution sur NPU. Il doit rester sur CPU (ou GPU), tandis que Hailo ne doit gÃ©rer que lâ€™encodeur. Cette sÃ©paration correspond dâ€™ailleurs naturellement Ã  la philosophie de Whisper : lâ€™encodeur extrait les caractÃ©ristiques acoustiques, alors que le dÃ©codeur effectue un travail de modÃ©lisation linguistique.
 Laisser la partie linguistique au CPU ne pÃ©nalise pas le temps rÃ©el.

### **Direction future**

La prochaine Ã©tape consiste donc Ã  revenir au cÅ“ur du problÃ¨me : la reconstruction complÃ¨te de lâ€™encodeur. Ce nouvel encodeur devra respecter strictement les contraintes du compilateur Hailo : mÃªme longueur temporelle que Whisper-small (idÃ©alement 1500), formes entiÃ¨rement dÃ©terministes, et aucune opÃ©ration susceptible dâ€™Ãªtre rÃ©Ã©crite par le compilateur.

Cela implique que je dois mâ€™appuyer davantage sur des convolutions classiques, des DepthwiseConv, du GroupNorm et des opÃ©rations Ã©lÃ©mentaires, tout en Ã©vitant les reshapes complexes, les broadcasts dynamiques et les sommes irrÃ©guliÃ¨res. Cette approche se rapproche davantage de la philosophie des architectures type YOLO, optimisÃ©es pour les NPUs embarquÃ©s.

Une fois que lâ€™encodeur aura Ã©tÃ© redÃ©fini, je suivrai la mÃªme procÃ©dure quâ€™auparavant : export en ONNX, conversion vers HAR, puis compilation en HEF. Si cette chaÃ®ne passe sans modifications non dÃ©sirÃ©es, je pourrai procÃ©der Ã  la distillation depuis Whisper. Le rÃ©sultat sera ensuite rÃ©utilisÃ© directement avec le dÃ©codeur de Whisper.

Du point de vue purement technique, jâ€™ai dÃ©jÃ  mis en place une pipeline de gÃ©nÃ©ration de donnÃ©es trÃ¨s fiable. GrÃ¢ce Ã  un service gRPC, je peux rÃ©cupÃ©rer de maniÃ¨re stable les sorties de lâ€™encodeur sur Hailo (vecteurs latents), et jâ€™ai constituÃ© un corpus de 50 000 exemples destinÃ©s Ã  lâ€™apprentissage du dÃ©codeur. Cette partie est fonctionnelle et robuste.
 Les problÃ¨mes actuels se concentrent donc sur deux points essentiels : la structure interne de lâ€™encodeur et la nÃ©cessitÃ©, lors de la distillation, de conserver une correspondance parfaite avec le contexte temporel de Whisper.

Ã€ lâ€™avenir, mon travail consistera Ã  construire un encodeur mieux adaptÃ© au hardware, dont la structure restera intacte aprÃ¨s compilation, qui pourra recevoir correctement la distillation du modÃ¨le maÃ®tre et qui permettra enfin la rÃ©alisation dâ€™un vÃ©ritable Â« Whisper embarquÃ© Â» sur Hailo-8L.

(Note : le flux complet du compilateur Hailo repose sur trois Ã©tapes : conversion, quantification et compilation. La quantification peut dÃ©jÃ  poser problÃ¨me, car le compilateur gÃ©nÃ¨re parfois des opÃ©rations inutiles. Lorsque jâ€™avais tentÃ© de compiler un ancien encodeur, jâ€™avais dÃ» contourner la dÃ©tection de bruit pour que le modÃ¨le passe la quantification. Par la suite, la compilation du fichier decoder.har sâ€™Ã©tait soldÃ©e par un Ã©chec, ce qui est cohÃ©rent avec la complexitÃ© extrÃªme du graphe du dÃ©codeur. Je mets donc cette partie en pause afin de me concentrer entiÃ¨rement sur lâ€™encodeur.)